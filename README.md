# Spark Project: CSV Data Processing with PySpark

## Overview
This project demonstrates how to use **Apache Spark (PySpark)** for processing Excel and other structured data.  
It covers:
- Reading Excel files into Spark DataFrames
- Performing Spark SQL transformations
- Writing results to CSV/Parquet
- Running locally with `spark-submit`

## Tech Stack
- **Apache Spark** (PySpark, Spark SQL)
- **Python 3.x**
- **OpenPyXL / Spark-Excel connector** (for Excel support)
- **VS Code / Jupyter Notebook**

## Project Structure

## ⚡ Features
- Load and preprocess data from CSV, JSON, and Parquet files.
- Perform Spark transformations (map, filter, reduceByKey, join).
- SQL queries on structured data.
- Machine learning pipeline (optional).
- Save results back to files or databases.

## ▶️ Setup & Installation

### Prerequisites
- Install [Apache Spark](https://spark.apache.org/downloads.html)  
- Install Python 3.x (if using PySpark)  
- Install Java 8/11 and set `JAVA_HOME`  

### Installation
Clone the repository:
```bash
git clone https://github.com/your-username/your-spark-project.git
cd your-spark-project
